{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "# open web browser\n",
    "\n",
    "def configure_driver():\n",
    "    # incognito window\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    \n",
    "    # add the argument and make the browser Headless.\n",
    "    # Chrome does not work well with headless\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    # open web browser\n",
    "    driver = webdriver.Chrome('/Users/abrahamleung/Documents/chromedriver', options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of categories\n",
    "categories = [\n",
    "    'activewear',\n",
    "    'jackets',\n",
    "    'sweatshirts-hoodies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('data/activewear.csv')"
   ]
  },
  {
   "source": [
    "# 3.2 Amazon Search Engine Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open web browser\n",
    "driver = configure_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search engine\n",
    "search_engine = 'https://www.amazon.com/?currency=HKD&language=en_US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input website\n",
    "driver.get(search_engine)\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose HKD\n",
    "\n",
    "# select option\n",
    "option = driver.find_element_by_id('icp-touch-link-cop')\n",
    "option.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select currency setting\n",
    "currency = driver.find_element_by_xpath('//span[@class=\"a-button-inner\"]')\n",
    "currency.click()\n",
    "\n",
    "# select hkd as currency\n",
    "hkd = driver.find_element_by_xpath('//li[@class=\"a-dropdown-item\"]/a[contains(text(), \"Hong Kong Dollar\")]')\n",
    "hkd.click()\n",
    "\n",
    "# save change\n",
    "save = driver.find_element_by_id('icp-btn-save')\n",
    "save.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 'women's fashion' as department\n",
    "\n",
    "department = driver.find_element_by_xpath('//select[@class=\"nav-search-dropdown searchSelect nav-progressive-attrubute nav-progressive-search-dropdown\"]/option[contains(text(), \"Women\\'s Fashion\")]')\n",
    "department.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (unsuccessful) create function\n",
    "# driver cannot serve as a variable\n",
    "# it means the driver cannot be input nor output through a function\n",
    "\n",
    "# Amazon search engine setup\n",
    "\n",
    "def amazon_setup():\n",
    "\n",
    "    # open web browser\n",
    "    driver = configure_driver()\n",
    "\n",
    "    # search engine\n",
    "    search_engine = 'https://www.amazon.com/?currency=HKD&language=en_US'\n",
    "\n",
    "    # input website\n",
    "    driver.get(search_engine)\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Choose HKD\n",
    "\n",
    "    # select option\n",
    "    option = driver.find_element_by_id('icp-touch-link-cop')\n",
    "    option.click()\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # select currency setting\n",
    "    currency = driver.find_element_by_xpath('//span[@class=\"a-button-inner\"]')\n",
    "    currency.click()\n",
    "\n",
    "    # select hkd as currency\n",
    "    hkd = driver.find_element_by_xpath(\n",
    "        '//li[@class=\"a-dropdown-item\"]/a[contains(text(), \"Hong Kong Dollar\")]'\n",
    "    )\n",
    "    hkd.click()\n",
    "\n",
    "    # save change\n",
    "    save = driver.find_element_by_id('icp-btn-save')\n",
    "    save.click()\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # select 'women's fashion' as department\n",
    "    department = driver.find_element_by_xpath(\n",
    "        '//select[@class=\"nav-search-dropdown searchSelect nav-progressive-attrubute nav-progressive-search-dropdown\"]/option[contains(text(), \"Women\\'s Fashion\")]'\n",
    "    )\n",
    "    department.click()\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "source": [
    "# 3.3 Scraping of Every Link"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input search item\n",
    "\n",
    "search_bar = driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_bar.send_keys(key_words)\n",
    "search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links\n",
    "\n",
    "links = []\n",
    "link_paths = driver.find_elements_by_xpath('//div/h2/a[@class=\"a-link-normal a-text-normal\"]')\n",
    "for link in link_paths:\n",
    "    x = link.get_attribute('href')\n",
    "    links.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "# scraping every link\n",
    "\n",
    "def all_links(driver, key_words):\n",
    "\n",
    "    # input search item\n",
    "    search_bar = driver.find_element_by_id('twotabsearchtextbox')\n",
    "    search_bar.send_keys(key_words)\n",
    "    search_bar.send_keys(Keys.ENTER)\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # get links\n",
    "    links = []\n",
    "    link_paths = driver.find_elements_by_xpath('//div/h2/a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for link in link_paths:\n",
    "        x = link.get_attribute('href')\n",
    "        links.append(x)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "source": [
    "# 3.4 Scraping of Search Result Details"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.amazon.com/CUPSHE-Womens-Floral-Halter-Swimsuit/dp/B07S223TYL/ref=sr_1_1?dchild=1&keywords=black+cutout+print+bikini&qid=1624866037&s=fashion-womens-intl-ship&sr=1-1'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# testing\n",
    "# url\n",
    "\n",
    "URL = links[0]\n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input website\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product names\n",
    "try:\n",
    "    name = driver.find_elements_by_tag_name('h1')\n",
    "    name = name[0].text\n",
    "except:\n",
    "    name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"CUPSHE Women's One Piece Swimsuit Cutout Halter Lace Up Twist Bathing Suit\""
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product price\n",
    "try:\n",
    "    price = driver.find_elements_by_xpath('//td[@class=\"a-span12\"]')\n",
    "    price = price[0].text\n",
    "except:\n",
    "    price = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'HKD 237.32'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product image\n",
    "try:\n",
    "    # size\n",
    "    img_width,img_height = 300,300\n",
    "\n",
    "    # get image\n",
    "    img = driver.find_elements_by_xpath('//img[@id=\"landingImage\"]')\n",
    "    img = img[0]\n",
    "\n",
    "    # 'src' = get image source\n",
    "    src = img.get_attribute('src')\n",
    "\n",
    "    # download image\n",
    "    urllib.request.urlretrieve(src, f'image/Amazon/{index}.png')\n",
    "\n",
    "    # resize image (smaller size)\n",
    "    ori_img = Image.open(f'image/Amazon/{index}.png')\n",
    "    resize_img = ori_img.resize((img_width,img_height))\n",
    "    resize_img.save(f'image/Amazon/{index}.png')\n",
    "\n",
    "    img_file = f'image/Amazon/{index}.png'\n",
    "except:\n",
    "    img_file = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "# search result details\n",
    "\n",
    "def scrape_details(index, link):\n",
    "\n",
    "    # input website\n",
    "    driver.get(link)\n",
    "\n",
    "    # product names\n",
    "    try:\n",
    "        name = driver.find_elements_by_tag_name('h1')\n",
    "        name = name[0].text\n",
    "    except:\n",
    "        name = None\n",
    "\n",
    "    # product price\n",
    "    try:\n",
    "        price = driver.find_elements_by_xpath('//td[@class=\"a-span12\"]')\n",
    "        price = price[0].text\n",
    "    except:\n",
    "        price = None\n",
    "    \n",
    "    # product image\n",
    "    try:\n",
    "        # size\n",
    "        img_width,img_height = 300,300\n",
    "\n",
    "        # get image\n",
    "        img = driver.find_elements_by_xpath('//img[@id=\"landingImage\"]')\n",
    "        img = img[0]\n",
    "\n",
    "        # 'src' = get image source\n",
    "        src = img.get_attribute('src')\n",
    "\n",
    "        # download image\n",
    "        urllib.request.urlretrieve(src, f'image/Amazon/{index}.png')\n",
    "\n",
    "        # resize image (smaller size)\n",
    "        ori_img = Image.open(f'image/Amazon/{index}.png')\n",
    "        resize_img = ori_img.resize((img_width,img_height))\n",
    "        resize_img.save(f'image/Amazon/{index}.png')\n",
    "\n",
    "        img_file = f'image/Amazon/{index}.png'\n",
    "    except:\n",
    "        img_file = None\n",
    "\n",
    "    return name, price, img_file"
   ]
  },
  {
   "source": [
    "# scrape every link\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "img_files = []\n",
    "index = 0\n",
    "\n",
    "for link in links:\n",
    "    \n",
    "    # scrape name, price and img_file\n",
    "    name, price, img_file = scrape_details(index, link)\n",
    "\n",
    "    # append to lists\n",
    "    names.append(name)\n",
    "    prices.append(price)\n",
    "    img_files.append(img_file)\n",
    "\n",
    "    # update index\n",
    "    index += 1\n",
    "\n",
    "# create dataframe\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'price': prices,\n",
    "    'img_file': img_files,\n",
    "    'url': links\n",
    "})"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# 3.5 Product Recommendations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# as no model training required, set 'compile=False'\n",
    "\n",
    "model = load_model('cnn_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "# feature extraction for image\n",
    "\n",
    "def get_embedding(model, img_file):\n",
    "\n",
    "    # reshape\n",
    "    img = image.load_img(img_file, target_size=(224, 224))\n",
    "\n",
    "    # img to numpy array\n",
    "    x = image.img_to_array(img)\n",
    "\n",
    "    # expand dim (1, w, h)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Preprocess input\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    return model.predict(x).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that get product recommendations based on the cosine similarity score of product image\n",
    "\n",
    "def get_recommender(idx, df, similarity, top_n):\n",
    "\n",
    "    # create dataframe for indices\n",
    "    indices = pd.Series(range(len(df)), index=df.index)\n",
    "\n",
    "    # select the target image (default is index 0)\n",
    "    target_index = indices[idx]\n",
    "\n",
    "    # calculate the similarity scores\n",
    "    similarity_scores = list(enumerate(similarity[target_index]))\n",
    "\n",
    "    # sort the result \n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # select the top n items\n",
    "    similarity_scores = similarity_scores[1:top_n+1]\n",
    "\n",
    "    # select the indices of top n items\n",
    "    top_indices = [i[0] for i in similarity_scores]\n",
    "\n",
    "    # idx_sim = [i[1] for i in sim_scores]\n",
    "\n",
    "    return indices.iloc[top_indices].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe duplicate\n",
    "df_copy = df\n",
    "\n",
    "# get image embeddings for each image\n",
    "map_embeddings = df_copy['img_file'].apply(lambda img: get_embedding(model, img))\n",
    "\n",
    "# convert series of lists to dataframe\n",
    "df_embeddings = map_embeddings.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       0          1         2         3         4         5         6     \\\n",
       "0  0.000000  16.656422  1.759933  0.396486  5.663714  0.000000  1.727282   \n",
       "1  0.000000   7.160003  1.574659  4.338456  2.551096  2.673633  4.204260   \n",
       "2  1.831643  22.057829  2.530438  1.242392  0.000000  0.000000  1.320725   \n",
       "3  3.890997   2.239640  0.561372  6.952099  2.865389  3.875372  6.084309   \n",
       "4  0.479622   3.835210  3.169219  5.755448  1.441075  5.746672  3.464339   \n",
       "\n",
       "       7         8         9     ...       2038      2039      2040  \\\n",
       "0  0.442207  0.000000  0.000000  ...   0.000000  0.000000  2.258237   \n",
       "1  6.550857  0.942263  2.079585  ...   0.975850  0.997066  2.406110   \n",
       "2  3.649347  0.030288  0.000000  ...   7.023321  2.911746  0.000000   \n",
       "3  2.111632  2.495424  4.386785  ...   0.406559  2.324595  1.143666   \n",
       "4  4.615533  4.365491  2.138535  ...  12.700304  0.725670  2.027468   \n",
       "\n",
       "        2041      2042  2043       2044      2045      2046       2047  \n",
       "0   2.662622  3.287225   0.0  12.032290  3.004779  0.000000   1.093109  \n",
       "1  25.856796  3.466363   0.0   7.697368  0.309879  2.489410   7.677667  \n",
       "2   1.198196  2.623420   0.0   2.178410  1.167908  3.153160  12.983468  \n",
       "3  17.603485  2.035505   0.0   1.155617  2.491142  0.000000   5.401934  \n",
       "4  11.041115  0.000000   0.0   0.997130  1.629200  1.458257  10.393693  \n",
       "\n",
       "[5 rows x 2048 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2038</th>\n      <th>2039</th>\n      <th>2040</th>\n      <th>2041</th>\n      <th>2042</th>\n      <th>2043</th>\n      <th>2044</th>\n      <th>2045</th>\n      <th>2046</th>\n      <th>2047</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>16.656422</td>\n      <td>1.759933</td>\n      <td>0.396486</td>\n      <td>5.663714</td>\n      <td>0.000000</td>\n      <td>1.727282</td>\n      <td>0.442207</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.258237</td>\n      <td>2.662622</td>\n      <td>3.287225</td>\n      <td>0.0</td>\n      <td>12.032290</td>\n      <td>3.004779</td>\n      <td>0.000000</td>\n      <td>1.093109</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>7.160003</td>\n      <td>1.574659</td>\n      <td>4.338456</td>\n      <td>2.551096</td>\n      <td>2.673633</td>\n      <td>4.204260</td>\n      <td>6.550857</td>\n      <td>0.942263</td>\n      <td>2.079585</td>\n      <td>...</td>\n      <td>0.975850</td>\n      <td>0.997066</td>\n      <td>2.406110</td>\n      <td>25.856796</td>\n      <td>3.466363</td>\n      <td>0.0</td>\n      <td>7.697368</td>\n      <td>0.309879</td>\n      <td>2.489410</td>\n      <td>7.677667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.831643</td>\n      <td>22.057829</td>\n      <td>2.530438</td>\n      <td>1.242392</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.320725</td>\n      <td>3.649347</td>\n      <td>0.030288</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>7.023321</td>\n      <td>2.911746</td>\n      <td>0.000000</td>\n      <td>1.198196</td>\n      <td>2.623420</td>\n      <td>0.0</td>\n      <td>2.178410</td>\n      <td>1.167908</td>\n      <td>3.153160</td>\n      <td>12.983468</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.890997</td>\n      <td>2.239640</td>\n      <td>0.561372</td>\n      <td>6.952099</td>\n      <td>2.865389</td>\n      <td>3.875372</td>\n      <td>6.084309</td>\n      <td>2.111632</td>\n      <td>2.495424</td>\n      <td>4.386785</td>\n      <td>...</td>\n      <td>0.406559</td>\n      <td>2.324595</td>\n      <td>1.143666</td>\n      <td>17.603485</td>\n      <td>2.035505</td>\n      <td>0.0</td>\n      <td>1.155617</td>\n      <td>2.491142</td>\n      <td>0.000000</td>\n      <td>5.401934</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.479622</td>\n      <td>3.835210</td>\n      <td>3.169219</td>\n      <td>5.755448</td>\n      <td>1.441075</td>\n      <td>5.746672</td>\n      <td>3.464339</td>\n      <td>4.615533</td>\n      <td>4.365491</td>\n      <td>2.138535</td>\n      <td>...</td>\n      <td>12.700304</td>\n      <td>0.725670</td>\n      <td>2.027468</td>\n      <td>11.041115</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.997130</td>\n      <td>1.629200</td>\n      <td>1.458257</td>\n      <td>10.393693</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 2048 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute similarity between items\n",
    "# calculate distance matrix\n",
    "similarity = 1 - pairwise_distances(df_embeddings, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.        , 0.5165263 , 0.5369592 , 0.51267225, 0.5224726 ,\n",
       "       0.5031241 , 0.5237402 , 0.51745456, 0.5408669 , 0.47479218,\n",
       "       0.48267484, 0.46357816, 0.50001   , 0.5165371 , 0.5172481 ,\n",
       "       0.58180815, 0.49073946, 0.5257547 , 0.5269011 , 0.5430988 ,\n",
       "       0.55302197, 0.57794875, 0.5942491 , 0.49452055, 0.5181655 ,\n",
       "       0.5339118 , 0.552034  , 0.5226602 , 0.5101911 , 0.49412894,\n",
       "       0.52358085, 0.5315855 , 0.51819026, 0.4978131 , 0.5014058 ,\n",
       "       0.47400868, 0.5076667 , 0.5177233 , 0.494326  , 0.48988938,\n",
       "       0.50520056, 0.5170815 , 0.49531442, 0.5356687 , 0.52722985,\n",
       "       0.5314058 , 0.5180114 , 0.50884783, 0.4995228 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "similarity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for indices\n",
    "indices = pd.Series(range(len(df)), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute recommendations' indices\n",
    "recommend_indices = get_recommender(0, df, 2)\n",
    "\n",
    "# create lists\n",
    "recommend_names = []\n",
    "recommend_prices = []\n",
    "recommend_img_files = []\n",
    "recommend_urls = []\n",
    "\n",
    "for i in recommend_indices:\n",
    "    # recommend_names.append(df.iloc[i]['name'])\n",
    "    # recommend_prices.append(df.iloc[i]['price'])\n",
    "    # recommend_urls.append(df.iloc[i]['url'])\n",
    "\n",
    "    # save the img_files separately\n",
    "    # use 'i-1' because the target item was appended at the front of dataframe\n",
    "    # the corresponding image index has to minus 1\n",
    "    img = Image.open(f'image/Amazon/{i-1}.png')\n",
    "\n",
    "    img_name = df.iloc[0]['img_file'][6:-4] + str(i) + '.png'\n",
    "    img.save(f'image/recommendations/{img_name}.png')\n",
    "\n",
    "    img_file = f'image/recommendations/{img_name}.png'\n",
    "\n",
    "    # append img_file's new path\n",
    "    recommend_img_files.append(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create products recommender\n",
    "\n",
    "def product_recommender(i,ck_target,df_amazon,model):\n",
    "    df_target = pd.DataFrame({\n",
    "        'name': [ck_target['name']],\n",
    "        'price': [ck_target['price']],\n",
    "        'img_file': [ck_target['img_file']],\n",
    "        'url': [ck_target['url']]\n",
    "    })\n",
    "\n",
    "    # append amazon dataframe to the target dataframe\n",
    "    df = df_target.append(df_amazon, ignore_index=True)\n",
    "\n",
    "    # make a dataframe duplicate\n",
    "    df_copy = df\n",
    "\n",
    "    # get image embeddings for each image\n",
    "    map_embeddings = df_copy['img_file'].apply(lambda img: get_embedding(model, img))\n",
    "\n",
    "    # convert series of lists to dataframe\n",
    "    df_embeddings = map_embeddings.apply(pd.Series)\n",
    "\n",
    "    ## compute similarity between items\n",
    "    # calculate distance matrix\n",
    "    similarity = 1 - pairwise_distances(df_embeddings, metric='cosine')\n",
    "\n",
    "    # create dataframe for indices\n",
    "    indices = pd.Series(range(len(df)), index=df.index)\n",
    "\n",
    "    # compute recommendations' indices\n",
    "    recommend_indices = get_recommender(0, df, similarity, 2)\n",
    "\n",
    "    # create lists\n",
    "    recommend_names = []\n",
    "    recommend_prices = []\n",
    "    recommend_img_files = []\n",
    "    recommend_urls = []\n",
    "\n",
    "    for j in recommend_indices:\n",
    "        recommend_names.append(df.iloc[j]['name'])\n",
    "        recommend_prices.append(df.iloc[j]['price'])\n",
    "        recommend_urls.append(df.iloc[j]['url'])\n",
    "\n",
    "        # save the img_files separately\n",
    "        # use 'i-1' because the target item was appended at the front of dataframe\n",
    "        # the corresponding image index has to minus 1\n",
    "        img = Image.open(f'image/Amazon/{j-1}.png')\n",
    "\n",
    "        img_name = ck_target['name'] + str(j)\n",
    "        img.save(f'image/recommendations/{img_name}.png')\n",
    "\n",
    "        img_file = f'image/recommendations/{img_name}.png'\n",
    "\n",
    "        # append img_file's new path\n",
    "        recommend_img_files.append(img_file)\n",
    "    return recommend_names, recommend_prices, recommend_img_files, recommend_urls"
   ]
  },
  {
   "source": [
    "# 3.6 Apply to All Categories"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3.6.1 Activewear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Amazon search engine setup\n",
    "\n",
    "# open web browser\n",
    "driver = configure_driver()\n",
    "\n",
    "# search engine\n",
    "search_engine = 'https://www.amazon.com/?currency=HKD&language=en_US'\n",
    "\n",
    "# input website\n",
    "driver.get(search_engine)\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Choose HKD\n",
    "\n",
    "# select option\n",
    "option = driver.find_element_by_id('icp-touch-link-cop')\n",
    "option.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select currency setting\n",
    "currency = driver.find_element_by_xpath('//span[@class=\"a-button-inner\"]')\n",
    "currency.click()\n",
    "\n",
    "# select hkd as currency\n",
    "hkd = driver.find_element_by_xpath(\n",
    "    '//li[@class=\"a-dropdown-item\"]/a[contains(text(), \"Hong Kong Dollar\")]'\n",
    ")\n",
    "hkd.click()\n",
    "\n",
    "# save change\n",
    "save = driver.find_element_by_id('icp-btn-save')\n",
    "save.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select 'women's fashion' as department\n",
    "department = driver.find_element_by_xpath(\n",
    "    '//select[@class=\"nav-search-dropdown searchSelect nav-progressive-attrubute nav-progressive-search-dropdown\"]/option[contains(text(), \"Women\\'s Fashion\")]'\n",
    ")\n",
    "department.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1/12) finished, execution time: 118.63s\n",
      "(2/12) finished, execution time: 134.76s\n",
      "(3/12) finished, execution time: 130.51s\n",
      "(4/12) finished, execution time: 81.45s\n",
      "(5/12) finished, execution time: 125.91s\n",
      "(6/12) finished, execution time: 136.77s\n",
      "(7/12) finished, execution time: 124.14s\n",
      "(8/12) finished, execution time: 136.37s\n",
      "(9/12) finished, execution time: 128.9s\n",
      "(10/12) finished, execution time: 130.68s\n",
      "(11/12) finished, execution time: 39.93s\n",
      "(12/12) finished, execution time: 138.3s\n",
      "Execution Completed\n"
     ]
    }
   ],
   "source": [
    "# activewear\n",
    "category = 'activewear'\n",
    "    \n",
    "# load data\n",
    "df_ck = pd.read_csv(f'data/{category}.csv')     \n",
    "\n",
    "list_recommend_names = []\n",
    "list_recommend_prices = []\n",
    "list_recommend_img_files = []\n",
    "list_recommend_urls = []\n",
    "\n",
    "## scraping all links\n",
    "for i,key_words in enumerate(df_ck['key_words']):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # input website\n",
    "    URL = f'https://www.amazon.com/s?k={key_words}&i=fashion-womens-intl-ship&ref=nb_sb_noss_2'\n",
    "    driver.get(URL)\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # get links\n",
    "    links = []\n",
    "    link_paths = driver.find_elements_by_xpath('//div/h2/a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for link in link_paths:\n",
    "        x = link.get_attribute('href')\n",
    "        links.append(x)\n",
    "    \n",
    "    ## scrape every product details\n",
    "\n",
    "    names = []\n",
    "    prices = []\n",
    "    img_files = []\n",
    "    index = 0\n",
    "\n",
    "    for link in links:\n",
    "        \n",
    "        # input website\n",
    "        driver.get(link)\n",
    "\n",
    "        # product names\n",
    "        try:\n",
    "            name = driver.find_elements_by_tag_name('h1')\n",
    "            name = name[0].text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        # product price\n",
    "        try:\n",
    "            price = driver.find_elements_by_xpath('//td[@class=\"a-span12\"]')\n",
    "            price = price[0].text\n",
    "        except:\n",
    "            price = None\n",
    "        \n",
    "        # product image\n",
    "        try:\n",
    "            # size\n",
    "            img_width,img_height = 300,300\n",
    "\n",
    "            # get image\n",
    "            img = driver.find_elements_by_xpath('//img[@id=\"landingImage\"]')\n",
    "            img = img[0]\n",
    "\n",
    "            # 'src' = get image source\n",
    "            src = img.get_attribute('src')\n",
    "\n",
    "            # download image\n",
    "            urllib.request.urlretrieve(src, f'image/Amazon/{index}.png')\n",
    "\n",
    "            # resize image (smaller size)\n",
    "            ori_img = Image.open(f'image/Amazon/{index}.png')\n",
    "            resize_img = ori_img.resize((img_width,img_height))\n",
    "            resize_img.save(f'image/Amazon/{index}.png')\n",
    "\n",
    "            img_file = f'image/Amazon/{index}.png'\n",
    "        except:\n",
    "            img_file = None\n",
    "\n",
    "        # append to lists\n",
    "        names.append(name)\n",
    "        prices.append(price)\n",
    "        img_files.append(img_file)\n",
    "\n",
    "        # update index\n",
    "        index += 1\n",
    "\n",
    "    # create dataframe\n",
    "    df_amazon = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'price': prices,\n",
    "        'img_file': img_files,\n",
    "        'url': links\n",
    "    })\n",
    "\n",
    "    # remove row with missing values\n",
    "    df_amazon.dropna(inplace=True)\n",
    "    df_amazon.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # products recommender\n",
    "    recommend_names, recommend_prices, recommend_img_files, recommend_urls = product_recommender(\n",
    "        i,df_ck.iloc[i],df_amazon,model\n",
    "    )\n",
    "\n",
    "    ## output lists for creating dataframe for final deployment use\n",
    "    list_recommend_names.append(recommend_names)\n",
    "    list_recommend_prices.append(recommend_prices)\n",
    "    list_recommend_img_files.append(recommend_img_files)\n",
    "    list_recommend_urls.append(recommend_urls)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'({i+1}/{len(df_ck)}) finished, execution time: {round(end_time - start_time,2)}s')\n",
    "\n",
    "print('Execution Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "names = df_ck['name'].to_list()\n",
    "prices = df_ck['price'].to_list()\n",
    "img_files = df_ck['img_file'].to_list()\n",
    "urls = df_ck['url'].to_list()\n",
    "\n",
    "df_recommender = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'price': prices,\n",
    "    'img_file': img_files,\n",
    "    'url': urls,\n",
    "    'recommend_names': list_recommend_names,\n",
    "    'recommend_prices': list_recommend_prices,\n",
    "    'recommend_img_files': list_recommend_img_files,\n",
    "    'recommend_urls': list_recommend_urls\n",
    "})\n",
    "\n",
    "# expanding recommendations into separate columns\n",
    "\n",
    "recommend_columns = ['recommend_names', 'recommend_prices', 'recommend_img_files', 'recommend_urls']\n",
    "\n",
    "for column in recommend_columns:\n",
    "    df_recommender[f'{column[:-1]}1'] = df_recommender[column].apply(lambda x: x[0])\n",
    "    df_recommender[f'{column[:-1]}2'] = df_recommender[column].apply(\n",
    "        lambda x: x[1] if len(x) == 2 else None\n",
    "    )\n",
    "    df_recommender.drop([column], axis=1, inplace=True)\n",
    "\n",
    "# data cleaning of 'price\n",
    "\n",
    "df_recommender['recommend_price1'] = df_recommender['recommend_price1'].apply(lambda x: x.split(' - ')[0])\n",
    "df_recommender['recommend_price2'] = df_recommender['recommend_price2'].apply(lambda x: x.split(' - ')[0])\n",
    "\n",
    "# save as csv\n",
    "df_recommender.to_csv(f'data/{category}_recommender.csv', index=False)"
   ]
  },
  {
   "source": [
    "# 3.6.2 Jackets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1/11) finished, execution time: 114.13s\n",
      "(2/11) finished, execution time: 40.65s\n",
      "(3/11) finished, execution time: 168.77s\n",
      "(4/11) finished, execution time: 134.43s\n",
      "(5/11) finished, execution time: 102.58s\n",
      "(6/11) finished, execution time: 152.66s\n",
      "(7/11) finished, execution time: 128.65s\n",
      "(8/11) finished, execution time: 146.5s\n",
      "(9/11) finished, execution time: 140.95s\n",
      "(10/11) finished, execution time: 108.46s\n",
      "(11/11) finished, execution time: 172.84s\n",
      "Execution Completed\n"
     ]
    }
   ],
   "source": [
    "## Amazon search engine setup\n",
    "\n",
    "# open web browser\n",
    "driver = configure_driver()\n",
    "\n",
    "# search engine\n",
    "search_engine = 'https://www.amazon.com/?currency=HKD&language=en_US'\n",
    "\n",
    "# input website\n",
    "driver.get(search_engine)\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Choose HKD\n",
    "\n",
    "# select option\n",
    "option = driver.find_element_by_id('icp-touch-link-cop')\n",
    "option.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select currency setting\n",
    "currency = driver.find_element_by_xpath('//span[@class=\"a-button-inner\"]')\n",
    "currency.click()\n",
    "\n",
    "# select hkd as currency\n",
    "hkd = driver.find_element_by_xpath(\n",
    "    '//li[@class=\"a-dropdown-item\"]/a[contains(text(), \"Hong Kong Dollar\")]'\n",
    ")\n",
    "hkd.click()\n",
    "\n",
    "# save change\n",
    "save = driver.find_element_by_id('icp-btn-save')\n",
    "save.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select 'women's fashion' as department\n",
    "department = driver.find_element_by_xpath(\n",
    "    '//select[@class=\"nav-search-dropdown searchSelect nav-progressive-attrubute nav-progressive-search-dropdown\"]/option[contains(text(), \"Women\\'s Fashion\")]'\n",
    ")\n",
    "department.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.1)\n",
    "\n",
    "# jackets\n",
    "category = 'jackets'\n",
    "    \n",
    "# load data\n",
    "df_ck = pd.read_csv(f'data/{category}.csv')     \n",
    "\n",
    "list_recommend_names = []\n",
    "list_recommend_prices = []\n",
    "list_recommend_img_files = []\n",
    "list_recommend_urls = []\n",
    "\n",
    "## scraping all links\n",
    "for i,key_words in enumerate(df_ck['key_words']):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # input website\n",
    "    URL = f'https://www.amazon.com/s?k={key_words}&i=fashion-womens-intl-ship&ref=nb_sb_noss_2'\n",
    "    driver.get(URL)\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # get links\n",
    "    links = []\n",
    "    link_paths = driver.find_elements_by_xpath('//div/h2/a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for link in link_paths:\n",
    "        x = link.get_attribute('href')\n",
    "        links.append(x)\n",
    "    \n",
    "    ## scrape every product details\n",
    "\n",
    "    names = []\n",
    "    prices = []\n",
    "    img_files = []\n",
    "    index = 0\n",
    "\n",
    "    for link in links:\n",
    "        \n",
    "        # input website\n",
    "        driver.get(link)\n",
    "\n",
    "        # product names\n",
    "        try:\n",
    "            name = driver.find_elements_by_tag_name('h1')\n",
    "            name = name[0].text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        # product price\n",
    "        try:\n",
    "            price = driver.find_elements_by_xpath('//td[@class=\"a-span12\"]')\n",
    "            price = price[0].text\n",
    "        except:\n",
    "            price = None\n",
    "        \n",
    "        # product image\n",
    "        try:\n",
    "            # size\n",
    "            img_width,img_height = 300,300\n",
    "\n",
    "            # get image\n",
    "            img = driver.find_elements_by_xpath('//img[@id=\"landingImage\"]')\n",
    "            img = img[0]\n",
    "\n",
    "            # 'src' = get image source\n",
    "            src = img.get_attribute('src')\n",
    "\n",
    "            # download image\n",
    "            urllib.request.urlretrieve(src, f'image/Amazon/{index}.png')\n",
    "\n",
    "            # resize image (smaller size)\n",
    "            ori_img = Image.open(f'image/Amazon/{index}.png')\n",
    "            resize_img = ori_img.resize((img_width,img_height))\n",
    "            resize_img.save(f'image/Amazon/{index}.png')\n",
    "\n",
    "            img_file = f'image/Amazon/{index}.png'\n",
    "        except:\n",
    "            img_file = None\n",
    "\n",
    "        # append to lists\n",
    "        names.append(name)\n",
    "        prices.append(price)\n",
    "        img_files.append(img_file)\n",
    "\n",
    "        # update index\n",
    "        index += 1\n",
    "\n",
    "    # create dataframe\n",
    "    df_amazon = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'price': prices,\n",
    "        'img_file': img_files,\n",
    "        'url': links\n",
    "    })\n",
    "\n",
    "    # remove row with missing values\n",
    "    df_amazon.dropna(inplace=True)\n",
    "    df_amazon.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # products recommender\n",
    "    recommend_names, recommend_prices, recommend_img_files, recommend_urls = product_recommender(\n",
    "        i,df_ck.iloc[i],df_amazon,model\n",
    "    )\n",
    "\n",
    "    ## output lists for creating dataframe for final deployment use\n",
    "    list_recommend_names.append(recommend_names)\n",
    "    list_recommend_prices.append(recommend_prices)\n",
    "    list_recommend_img_files.append(recommend_img_files)\n",
    "    list_recommend_urls.append(recommend_urls)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'({i+1}/{len(df_ck)}) finished, execution time: {round(end_time - start_time,2)}s')\n",
    "\n",
    "print('Execution Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "names = df_ck['name'].to_list()\n",
    "prices = df_ck['price'].to_list()\n",
    "img_files = df_ck['img_file'].to_list()\n",
    "urls = df_ck['url'].to_list()\n",
    "\n",
    "df_recommender = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'price': prices,\n",
    "    'img_file': img_files,\n",
    "    'url': urls,\n",
    "    'recommend_names': list_recommend_names,\n",
    "    'recommend_prices': list_recommend_prices,\n",
    "    'recommend_img_files': list_recommend_img_files,\n",
    "    'recommend_urls': list_recommend_urls\n",
    "})\n",
    "\n",
    "# expanding recommendations into separate columns\n",
    "\n",
    "recommend_columns = ['recommend_names', 'recommend_prices', 'recommend_img_files', 'recommend_urls']\n",
    "\n",
    "for column in recommend_columns:\n",
    "    df_recommender[f'{column[:-1]}1'] = df_recommender[column].apply(lambda x: x[0])\n",
    "    df_recommender[f'{column[:-1]}2'] = df_recommender[column].apply(\n",
    "        lambda x: x[1] if len(x) == 2 else None\n",
    "    )\n",
    "    df_recommender.drop([column], axis=1, inplace=True)\n",
    "\n",
    "# data cleaning of 'price\n",
    "\n",
    "df_recommender['recommend_price1'] = df_recommender['recommend_price1'].apply(lambda x: x.split(' - ')[0])\n",
    "df_recommender['recommend_price2'] = df_recommender['recommend_price2'].apply(lambda x: x.split(' - ')[0])\n",
    "\n",
    "# save as csv\n",
    "df_recommender.to_csv(f'data/{category}_recommender.csv', index=False)"
   ]
  },
  {
   "source": [
    "# 3.6.3 Sweatshirts-Hoodies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1/12) finished, execution time: 67.65s\n",
      "(2/12) finished, execution time: 162.56s\n",
      "(3/12) finished, execution time: 142.32s\n",
      "(4/12) finished, execution time: 141.14s\n",
      "(5/12) finished, execution time: 157.6s\n",
      "(6/12) finished, execution time: 127.39s\n",
      "(7/12) finished, execution time: 171.27s\n",
      "(8/12) finished, execution time: 154.03s\n",
      "(9/12) finished, execution time: 99.99s\n",
      "(10/12) finished, execution time: 134.48s\n",
      "(11/12) finished, execution time: 60.74s\n",
      "(12/12) finished, execution time: 18.78s\n",
      "Execution Completed\n"
     ]
    }
   ],
   "source": [
    "## Amazon search engine setup\n",
    "\n",
    "# open web browser\n",
    "driver = configure_driver()\n",
    "\n",
    "# search engine\n",
    "search_engine = 'https://www.amazon.com/?currency=HKD&language=en_US'\n",
    "\n",
    "# input website\n",
    "driver.get(search_engine)\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Choose HKD\n",
    "\n",
    "# select option\n",
    "option = driver.find_element_by_id('icp-touch-link-cop')\n",
    "option.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select currency setting\n",
    "currency = driver.find_element_by_xpath('//span[@class=\"a-button-inner\"]')\n",
    "currency.click()\n",
    "\n",
    "# select hkd as currency\n",
    "hkd = driver.find_element_by_xpath(\n",
    "    '//li[@class=\"a-dropdown-item\"]/a[contains(text(), \"Hong Kong Dollar\")]'\n",
    ")\n",
    "hkd.click()\n",
    "\n",
    "# save change\n",
    "save = driver.find_element_by_id('icp-btn-save')\n",
    "save.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.5)\n",
    "\n",
    "# select 'women's fashion' as department\n",
    "department = driver.find_element_by_xpath(\n",
    "    '//select[@class=\"nav-search-dropdown searchSelect nav-progressive-attrubute nav-progressive-search-dropdown\"]/option[contains(text(), \"Women\\'s Fashion\")]'\n",
    ")\n",
    "department.click()\n",
    "\n",
    "# may need time sleep\n",
    "time.sleep(0.1)\n",
    "\n",
    "# activewear\n",
    "category = 'sweatshirts-hoodies'\n",
    "    \n",
    "# load data\n",
    "df_ck = pd.read_csv(f'data/{category}.csv')     \n",
    "\n",
    "list_recommend_names = []\n",
    "list_recommend_prices = []\n",
    "list_recommend_img_files = []\n",
    "list_recommend_urls = []\n",
    "\n",
    "## scraping all links\n",
    "for i,key_words in enumerate(df_ck['key_words']):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # input website\n",
    "    URL = f'https://www.amazon.com/s?k={key_words}&i=fashion-womens-intl-ship&ref=nb_sb_noss_2'\n",
    "    driver.get(URL)\n",
    "\n",
    "    # may need time sleep\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # get links\n",
    "    links = []\n",
    "    link_paths = driver.find_elements_by_xpath('//div/h2/a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for link in link_paths:\n",
    "        x = link.get_attribute('href')\n",
    "        links.append(x)\n",
    "    \n",
    "    ## scrape every product details\n",
    "\n",
    "    names = []\n",
    "    prices = []\n",
    "    img_files = []\n",
    "    index = 0\n",
    "\n",
    "    for link in links:\n",
    "        \n",
    "        # input website\n",
    "        driver.get(link)\n",
    "\n",
    "        # product names\n",
    "        try:\n",
    "            name = driver.find_elements_by_tag_name('h1')\n",
    "            name = name[0].text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        # product price\n",
    "        try:\n",
    "            price = driver.find_elements_by_xpath('//td[@class=\"a-span12\"]')\n",
    "            price = price[0].text\n",
    "        except:\n",
    "            price = None\n",
    "        \n",
    "        # product image\n",
    "        try:\n",
    "            # size\n",
    "            img_width,img_height = 300,300\n",
    "\n",
    "            # get image\n",
    "            img = driver.find_elements_by_xpath('//img[@id=\"landingImage\"]')\n",
    "            img = img[0]\n",
    "\n",
    "            # 'src' = get image source\n",
    "            src = img.get_attribute('src')\n",
    "\n",
    "            # download image\n",
    "            urllib.request.urlretrieve(src, f'image/Amazon/{index}.png')\n",
    "\n",
    "            # resize image (smaller size)\n",
    "            ori_img = Image.open(f'image/Amazon/{index}.png')\n",
    "            resize_img = ori_img.resize((img_width,img_height))\n",
    "            resize_img.save(f'image/Amazon/{index}.png')\n",
    "\n",
    "            img_file = f'image/Amazon/{index}.png'\n",
    "        except:\n",
    "            img_file = None\n",
    "\n",
    "        # append to lists\n",
    "        names.append(name)\n",
    "        prices.append(price)\n",
    "        img_files.append(img_file)\n",
    "\n",
    "        # update index\n",
    "        index += 1\n",
    "\n",
    "    # create dataframe\n",
    "    df_amazon = pd.DataFrame({\n",
    "        'name': names,\n",
    "        'price': prices,\n",
    "        'img_file': img_files,\n",
    "        'url': links\n",
    "    })\n",
    "\n",
    "    # remove row with missing values\n",
    "    df_amazon.dropna(inplace=True)\n",
    "    df_amazon.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # products recommender\n",
    "    recommend_names, recommend_prices, recommend_img_files, recommend_urls = product_recommender(\n",
    "        i,df_ck.iloc[i],df_amazon,model\n",
    "    )\n",
    "\n",
    "    ## output lists for creating dataframe for final deployment use\n",
    "    list_recommend_names.append(recommend_names)\n",
    "    list_recommend_prices.append(recommend_prices)\n",
    "    list_recommend_img_files.append(recommend_img_files)\n",
    "    list_recommend_urls.append(recommend_urls)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'({i+1}/{len(df_ck)}) finished, execution time: {round(end_time - start_time,2)}s')\n",
    "\n",
    "print('Execution Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "names = df_ck['name'].to_list()\n",
    "prices = df_ck['price'].to_list()\n",
    "img_files = df_ck['img_file'].to_list()\n",
    "urls = df_ck['url'].to_list()\n",
    "\n",
    "df_recommender = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'price': prices,\n",
    "    'img_file': img_files,\n",
    "    'url': urls,\n",
    "    'recommend_names': list_recommend_names,\n",
    "    'recommend_prices': list_recommend_prices,\n",
    "    'recommend_img_files': list_recommend_img_files,\n",
    "    'recommend_urls': list_recommend_urls\n",
    "})\n",
    "\n",
    "# expanding recommendations into separate columns\n",
    "\n",
    "recommend_columns = ['recommend_names', 'recommend_prices', 'recommend_img_files', 'recommend_urls']\n",
    "\n",
    "for column in recommend_columns:\n",
    "    df_recommender[f'{column[:-1]}1'] = df_recommender[column].apply(lambda x: x[0])\n",
    "    df_recommender[f'{column[:-1]}2'] = df_recommender[column].apply(\n",
    "        lambda x: x[1] if len(x) == 2 else None\n",
    "    )\n",
    "    df_recommender.drop([column], axis=1, inplace=True)\n",
    "\n",
    "# data cleaning of 'price\n",
    "\n",
    "df_recommender['recommend_price1'] = df_recommender['recommend_price1'].apply(lambda x: x.split(' - ')[0])\n",
    "df_recommender['recommend_price2'] = df_recommender['recommend_price2'].apply(lambda x: x.split(' - ')[0])\n",
    "\n",
    "# save as csv\n",
    "df_recommender.to_csv(f'data/{category}_recommender.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "fa3fba418643e7663ba7576308ac3a337e9dc56c31f42a0bad2dca4c82d258a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}